apiVersion: template.openshift.io/v1
kind: Template
metadata:
  creationTimestamp: null
  name: airflow
objects:
- apiVersion: v1
  kind: RoleBinding
  metadata:
    name: airflow-cluster-access
  roleRef:
    name: edit
  subjects:
  - kind: ServiceAccount
    name: airflow-cluster-access
    namespace: ${AIRFLOW_NAMESPACE}
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      app: airflow-scheduler
    name: airflow-cluster-access
- apiVersion: v1
  kind: ConfigMap
  metadata:
    labels:
      app: airflow
    name: airflow-config
  data:
    airflow.cfg: |+
      [core]
      # The home folder for airflow, default is ~/airflow
      dag_default_view = graph
      # The folder where your airflow pipelines live, most likely a
      # subfolder in a code repository
      # This path must be absolute
      dags_folder = ${AIRFLOW_HOME}/dags
      # The folder where airflow should store its log files
      # This path must be absolute
      base_log_folder = ${AIRFLOW_HOME}/logs
      # Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users
      # must supply a remote location URL (starting with either 's3://...' or
      # 'gs://...') and an Airflow connection id that provides access to the storage
      # location.
      #remote_base_log_folder =
      #remote_log_conn_id =
      # Use server-side encryption for logs stored in S3
      #encrypt_s3_logs = False
      # DEPRECATED option for remote log storage, use remote_base_log_folder instead!
      #s3_log_folder =
      #Timezone used by airflow
      default_timezone = America/Toronto
      # The executor class that airflow should use. Choices include
      # SequentialExecutor, LocalExecutor, CeleryExecutor
      executor = KubernetesExecutor
      # The SqlAlchemy connection string to the metadata database.
      # SqlAlchemy supports many different database engine, more information
      # their website
      #sql_alchemy_conn=postgresql://airflow:Pth7cFgY2GDE83MG@ulvsecad23.devfg.example.com:5432/airflow
      #sql_alchemy_conn=postgresql://airflow:airflow@postgres:5432/airflow
      sql_alchemy_conn=mysql://airflow:airflow@mariadb:3306/airflow
      #sql_alchemy_con_cmd = "rsaw -d -k ${AIRFLOW_HOME}//keys/id_rsa_airflow -f asd"
      # If SqlAlchemy should pool database connections.
      sql_alchemy_pool_enabled = True
      # The SqlAlchemy pool size is the maximum number of database connections
      # in the pool.
      sql_alchemy_pool_size = 3
      # The SqlAlchemy pool recycle is the number of seconds a connection
      # can be idle in the pool before it is invalidated. This config does
      # not apply to sqlite.
      sql_alchemy_pool_recycle = 3600
      # The amount of parallelism as a setting to the executor. This defines
      # the max number of task instances that should run simultaneously
      # on this airflow installation
      parallelism = 32
      # The number of task instances allowed to run concurrently by the scheduler
      dag_concurrency = 16
      # Are DAGs paused by default at creation
      dags_are_paused_at_creation = True
      # When not using pools, tasks are run in the "default pool",
      # whose size is guided by this config element
      non_pooled_task_slot_count = 128
      # The maximum number of active DAG runs per DAG
      max_active_runs_per_dag = 16
      # Whether to load the examples that ship with Airflow. It's good to
      # get started, but you probably want to set this to False in a production
      # environment
      load_examples = False
      # Where your Airflow plugins are stored
      plugins_folder = ${AIRFLOW_HOME}/plugins
      # Secret key to save connection passwords in the db
      #fernet_key_cmd = "python3 -c 'from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)'"
      #fernet_key = 6KbLlGPzUZyby2W5d3afQW39fom-SC_Ses4AKJhMcKY=
      # Whether to disable pickling dags
      donot_pickle = False
      # How long before timing out a python file import while filling the DagBag
      dagbag_import_timeout = 30
      # The class to use for running task instances in a subprocess
      task_runner = StandardTaskRunner
      # If set, tasks without a `run_as_user` argument will be run with this user
      # Can be used to de-elevate a sudo user running Airflow when executing tasks
      #default_impersonation =
      # What security module to use (for example kerberos):
      #security =
      # Turn unit test mode on (overwrites many configuration options with test
      # values at runtime)
      #unit_test_mode = False
      [cli]
      # In what way should the cli access the API. The LocalClient will use the
      # database directly, while the json_client will use the api running on the
      # webserver
      api_client = airflow.api.client.local_client
      endpoint_url = http://localhost:8080
      [api]
      # How to authenticate users of the API
      auth_backend = airflow.api.auth.backend.default
      [operators]
      # The default owner assigned to each new operator, unless
      # provided explicitly or passed via `default_args`
      default_owner = Airflow
      default_cpus = 1
      default_ram = 512
      default_disk = 512
      default_gpus = 0
      [webserver]
      # The base url of your website as airflow cannot guess what domain or
      # cname you are using. This is used in automated emails that
      # airflow sends to point links to the right web server
      #base_url = http://ulvsecad23.devfg.example.com:8080
      # The ip specified when starting the web server
      #web_server_host = 0.0.0.0
      # The port on which to run the web server
      #web_server_port = 8080
      # Paths to the SSL certificate and key for the web server. When both are
      # provided SSL will be enabled. This does not change the web server port.
      #web_server_ssl_cert = ${AIRFLOW_HOME}/airflow/ssl/cert.pem
      #web_server_ssl_key = ${AIRFLOW_HOME}/airflow/ssl/key.pem
      # Number of seconds the gunicorn webserver waits before timing out on a worker
      #web_server_worker_timeout = 120
      # Number of workers to refresh at a time. When set to 0, worker refresh is
      # disabled. When nonzero, airflow periodically refreshes webserver workers by
      # bringing up new ones and killing old ones.
      #worker_refresh_batch_size = 1
      # Number of seconds to wait before refreshing a batch of workers.
      #worker_refresh_interval = 30
      # Secret key used to run your flask app
      #secret_key = temporary_key
      # Number of workers to run the Gunicorn web server
      #workers = 5
      # The worker class gunicorn should use. Choices include
      # sync (default), eventlet, gevent
      #worker_class = sync
      # Log files for the gunicorn webserver. '-' means log to stderr.
      #access_logfile = -
      #error_logfile = -
      # Expose the configuration file in the web server
      #expose_config = False
      # Set to true to turn on authentication:
      # http://pythonhosted.org/airflow/security.html#web-authentication
      #authenticate = True
      #auth_backend = airflow.contrib.auth.backends.password_auth
      #auth_backend = airflow.contrib.auth.backends.ldap_auth
      # Filter the list of dags by owner name (requires authentication to be enabled)
      #filter_by_owner = False
      # Filtering mode. Choices include user (default) and ldapgroup.
      # Ldap group filtering requires using the ldap backend
      #
      # Note that the ldap server needs the "memberOf" overlay to be set up
      # in order to user the ldapgroup mode.
      #owner_mode = user
      # Default DAG orientation. Valid values are:
      # LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
      #dag_orientation = LR
      # Puts the webserver in demonstration mode; blurs the names of Operators for
      # privacy.
      #demo_mode = False
      # The amount of time (in secs) webserver will wait for initial handshake
      # while fetching logs from other worker machine
      #log_fetch_timeout_sec = 5
      # By default, the webserver shows paused DAGs. Flip this to hide paused
      # DAGs by default
      #hide_paused_dags_by_default = False
      rbac = false
      #[ldap]
      # set a connection without encryption: uri = ldap://<your.ldap.server>:<port>
      #uri = ldaps://<your.ldap.server>:<port>
      #uri = ldaps://fg.example.com:3269
      #uri = ldap://ulviasp05854:33268
      #user_filter = objectClass=*
      # in case of Active Directory you would use: user_name_attr = sAMAccountName
      #user_name_attr = uid
      #user_name_attr = sAMAccountName
      # group_member_attr should be set accordingly with *_filter
      # eg :
      #     group_member_attr = groupMembership
      #     superuser_filter = groupMembership=CN=airflow-super-users...
      #group_member_attr = memberOf
      #superuser_filter = memberOf=CN=APP_URK0_FGH_cyber_AUTHL2,OU=PAM,OU=Universal Groups,OU=Accounts,DC=fg,DC=example,DC=com
      #data_profiler_filter = memberOf=CN=APP_RAN0_FGH_prod_HDP_read,OU=Universal Groups,OU=Accounts,DC=fg,DC=example,DC=com
      #bind_user = PFGHSRVLDAP@MAPLE.FG.example.COM
      #bind_password = Qm8hcrwnst5Kyf2
      #basedn = dc=maple,dc=fg,dc=example,dc=com
      #cacert = /etc/pki/ca-trust/source/anchors/PROD.ca-bundle
      # Set search_scope to one of them:  BASE, LEVEL , SUBTREE
      # Set search_scope to SUBTREE if using Active Directory, and not specifying an Organizational Unit
      #search_scope = SUBTREE
      #[email]
      #email_backend = airflow.utils.email.send_email_smtp
      #[smtp]
      # If you want airflow to send emails on retries, failure, and you want to use
      # the airflow.utils.email.send_email_smtp function, you have to configure an
      # smtp server here
      #smtp_host = labmailer.devfg.example.com
      #smtp_starttls = True
      #smtp_ssl = False
      # Uncomment and set the user/pass settings if you want to use SMTP AUTH
      # smtp_user = airflow
      # smtp_password = airflow
      #smtp_port = 25
      #smtp_mail_from = noreply-fgh-dev@example.com
      #[celery]
      # This section only applies if you are using the CeleryExecutor in
      # [core] section above
      # The app name that will be used by celery
      #celery_app_name = airflow.executors.celery_executor
      # The concurrency that will be used when starting workers with the
      # "airflow worker" command. This defines the number of task instances that
      # a worker will take, so size up your workers based on the resources on
      # your worker box and the nature of your tasks
      #worker_concurrency = 16
      # When you start an airflow worker, airflow starts a tiny web server
      # subprocess to serve the workers local log files to the airflow main
      # web server, who then builds pages and sends them to users. This defines
      # the port on which the logs are served. It needs to be unused, and open
      # visible from the main web server to connect into the workers.
      #worker_log_server_port = 8795
      # The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
      # a sqlalchemy database. Refer to the Celery documentation for more
      # information.
      #broker_url = redis://root:VXQZNQG4RFzzbS9M@ulvsecad23.devfg.example.com:6379/0
      # Another key Celery setting
      #result_backend = db+postgresql://airflow:Pth7cFgY2GDE83MG@ulvsecad23.devfg.example.com:5432/airflow
      # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
      # it `airflow flower`. This defines the IP that Celery Flower runs on
      #flower_host = 0.0.0.0
      # This defines the port that Celery Flower runs on
      #flower_port = 5555
      # Default queue that tasks get assigned to and that worker listen on.
      #default_queue = default
      #[celery_broker_transport_options]
      # The visibility timeout defines the number of seconds to wait for the worker
      # to acknowledge the task before the message is redelivered to another worker.
      # Make sure to increase the visibility timeout to match the time of the longest
      # ETA you're planning to use. Especially important in case of using Redis or SQS
      #visibility_timeout = 36000
      [scheduler]
      # Task instances listen for external kill signal (when you clear tasks
      # from the CLI or the UI), this defines the frequency at which they should
      # listen (in seconds).
      job_heartbeat_sec = 5
      # The scheduler constantly tries to trigger new tasks (look at the
      # scheduler section in the docs for more information). This defines
      # how often the scheduler should run (in seconds).
      scheduler_heartbeat_sec = 5
      # after how much time should the scheduler terminate in seconds
      # -1 indicates to run continuously (see also num_runs)
      run_duration = -1
      # after how much time a new DAGs should be picked up from the filesystem
      min_file_process_interval = 0
      dag_dir_list_interval = 300
      # How often should stats be printed to the logs
      print_stats_interval = 30
      child_process_log_directory = ${AIRFLOW_HOME}/airflow/logs/scheduler
      # Local task jobs periodically heartbeat to the DB. If the job has
      # not heartbeat in this many seconds, the scheduler will mark the
      # associated task instance as failed and will re-schedule the task.
      scheduler_zombie_task_threshold = 300
      # Turn off scheduler catchup by setting this to False.
      # Default behavior is unchanged and
      # Command Line Backfills still work, but the scheduler
      # will not do scheduler catchup if this is False,
      # however it can be set on a per DAG basis in the
      # DAG definition (catchup)
      catchup_by_default = False
      # Statsd (https://github.com/etsy/statsd) integration settings
      #statsd_on = False
      #statsd_host = localhost
      #statsd_port = 8125
      #statsd_prefix = airflow
      # The scheduler can run multiple threads in parallel to schedule dags.
      # This defines how many threads will run. However airflow will never
      # use more threads than the amount of cpu cores available.
      max_threads = 3
      #authenticate = True
      #[mesos]
      # Mesos master address which MesosExecutor will connect to.
      #master = localhost:5050
      # The framework name which Airflow scheduler will register itself as on mesos
      #framework_name = Airflow
      # Number of cpu cores required for running one task instance using
      # 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
      # command on a mesos slave
      #task_cpu = 1
      # Memory in MB required for running one task instance using
      # 'airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>'
      # command on a mesos slave
      #task_memory = 256
      # Enable framework checkpointing for mesos
      # See http://mesos.apache.org/documentation/latest/slave-recovery/
      #checkpoint = False
      # Failover timeout in milliseconds.
      # When checkpointing is enabled and this option is set, Mesos waits
      # until the configured timeout for
      # the MesosExecutor framework to re-register after a failover. Mesos
      # shuts down running tasks if the
      # MesosExecutor framework fails to re-register within this timeframe.
      # failover_timeout = 604800
      # Enable framework authentication for mesos
      # See http://mesos.apache.org/documentation/latest/configuration/
      #authenticate = True
      # Mesos credentials, if authentication is enabled
      # default_principal = admin
      # default_secret = admin
      #[github_enterprise]
      #api_rev = v3
      [admin]
      # UI to hide sensitive variable fields when set to True
      hide_sensitive_variable_fields = True
      [kubernetes]
      dags_in_image = True
      worker_container_repository = ${AIRFLOW_IMAGE_NAME}
      worker_container_tag = ${AIRFLOW_IMAGE_TAG}
      worker_container_image_pull_policy = Never
      delete_worker_pods = True
      in_cluster = true
      namespace = ${AIRFLOW_NAMESPACE}
      airflow_configmap = airflow-config
      run_as_user = 1001
      [kubernetes_environment_variables]
      AIRFLOW_HOME = ${AIRFLOW_HOME}
      AIRFLOW__CORE__EXECUTOR = KubernetesExecutor
- apiVersion: v1
  kind: ConfigMap
  metadata:
    labels:
      app: airflow
    name: airflow-init
  data:
    init-airflow.sh: |
      set -e
      export USER_ID=$(id -u)
      export GROUP_ID=$(id -g)
      export AIRFLOW_CONFIG=${AIRFLOW_HOME}/airflow.cfg
      echo "${USER_NAME:-default}:x:$(id -u):0:${USER_NAME:-default} user:${HOME}:/sbin/nologin" >> /etc/passwd
      AIRFLOW_PATH=`pip3 show apache-airflow | grep 'Location: ' | cut -d' ' -f2 || true`
        if [ -z $AIRFLOW_PY3 ]; then
          AIRFLOW_PATH=`pip show apache-airflow | grep 'Location: ' | cut -d' ' -f2`
        fi
        AIRFLOW_VERSION=`pip freeze | grep apache-airflow | cut -d'=' -f3`
      cd $AIRFLOW_PATH/airflow
      airflow upgradedb
      alembic upgrade heads
      if [[ "$AIRFLOW_VERSION" > "2" ]]; then
      CREATE_USER="users --create"
      else
      CREATE_USER="create_user"
      fi      
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: airflow-web
    name: airflow-web
  spec:
    ports:
    - name: web
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: airflow-web
      release: airflow
    type: NodePort
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: mariadb
    name: mariadb
  spec:
    ports:
    - name: 3306-tcp
      port: 3306
      protocol: TCP
      targetPort: 3306
    selector:
      app: airflow
      component: mariadb
    type: ClusterIP
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
    labels:
      app: airflow-scheduler
    name: airflow-scheduler
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow-scheduler
        release: airflow
    strategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 100%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app: airflow-scheduler
          release: airflow
      spec:
        containers:
        - args:
          - scheduler
          env:
          - name: AIRFLOW_KUBE_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          env:
          - name: AIRFLOW__CORE__EXECUTOR
            value: 'KubernetesExecutor'
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            value: 'mysql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:3306/${DB_AIRFLOW}'
          - name: AIRFLOW__KUBERNETES__KUBE_CLIENT_REQUEST_ARGS
            value: '{ "_request_timeout": "50" }'
          - name: AIRFLOW_HOME
            value: ${AIRFLOW_HOME}
          - name: MYSQL_HOST
            value: ${DB_HOST}
          - name: MYSQL_USER
            value: ${DB_USER}
          - name: MYSQL_PASSWORD
            value: ${DB_PASSWORD}
          - name: FERNET_KEY
            value: j0PNE8131Vx-ix7BsNDwskFUlnLa00mWU17BRujVcdY=
          image: ${AIRFLOW_IMAGE_NAME}:${AIRFLOW_IMAGE_TAG}
          imagePullPolicy: Never
          name: scheduler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: ${AIRFLOW_HOME}/airflow.cfg
            name: airflow-config
            subPath: airflow.cfg
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: airflow-cluster-access
        serviceAccountName: airflow-cluster-access
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-config
          name: airflow-config
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      app: airflow-web
    name: airflow-web
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow-web
        release: airflow
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        labels:
          app: airflow-web
          release: airflow
      spec:
        containers:
        - args:
          - webserver
          env:
          - name: AIRFLOW_CONFIG
            value: ${AIRFLOW_HOME}/airflow.cfg
          - name: AIRFLOW_KUBE_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          env:
          - name: AIRFLOW__CORE__EXECUTOR
            value: 'KubernetesExecutor'
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            value: 'mysql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:3306/${DB_AIRFLOW}'
          - name: AIRFLOW__KUBERNETES__KUBE_CLIENT_REQUEST_ARGS
            value: '{ "_request_timeout": "50" }'
          - name: AIRFLOW_HOME
            value: ${AIRFLOW_HOME}
          - name: MYSQL_HOST
            value: ${DB_HOST}
          - name: MYSQL_USER
            value: ${DB_USER}
          - name: MYSQL_PASSWORD
            value: ${DB_PASSWORD}
          - name: FERNET_KEY
            value: j0PNE8131Vx-ix7BsNDwskFUlnLa00mWU17BRujVcdY=
          image: ${AIRFLOW_IMAGE_NAME}:${AIRFLOW_IMAGE_TAG}
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: webserver
          ports:
          - containerPort: 8080
            name: webserver
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: ${AIRFLOW_HOME}/airflow.cfg
            name: airflow-config
            subPath: airflow.cfg
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - -cx
          - bash -x /entrypoint/init-airflow.sh
          command:
          - bash
          env:
          - name: AIRFLOW_CONFIG
            value: ${AIRFLOW_HOME}/airflow.cfg
          - name: AIRFLOW__CORE__EXECUTOR
            value: 'KubernetesExecutor'
          - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN
            value: 'mysql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:3306/${DB_AIRFLOW}'
          - name: AIRFLOW__KUBERNETES__KUBE_CLIENT_REQUEST_ARGS
            value: '{ "_request_timeout": "50" }'
          - name: AIRFLOW_HOME
            value: ${AIRFLOW_HOME}
          - name: MYSQL_HOST
            value: ${DB_HOST}
          - name: MYSQL_USER
            value: ${DB_USER}
          - name: MYSQL_PASSWORD
            value: ${DB_PASSWORD}
          - name: FERNET_KEY
            value: j0PNE8131Vx-ix7BsNDwskFUlnLa00mWU17BRujVcdY=
          image: ${AIRFLOW_IMAGE_NAME}:${AIRFLOW_IMAGE_TAG}
          imagePullPolicy: Never
          name: init-airflow
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: ${AIRFLOW_HOME}/airflow.cfg
            name: airflow-config
            subPath: airflow.cfg
          - mountPath: /entrypoint
            name: init-airflow
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: airflow-config
          name: airflow-config
        - configMap:
            defaultMode: 420
            name: airflow-init
          name: init-airflow
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      app: airflow
      component: mariadb
    name: mariadb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: airflow
        component: mariadb
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        labels:
          app: airflow
          component: mariadb
      spec:
        containers:
        - env:
          - name: MYSQL_USER
            value: ${DB_USER}
          - name: MYSQL_PASSWORD
            value: ${DB_PASSWORD}
          - name: MYSQL_DATABASE
            value: ${DB_AIRFLOW}
          - name: MYSQL_ROOT_PASSWORD
            value: ${DB_PASSWORD}
          - name: MYSQL_EXPLICIT_DEFAULTS_FOR_TIMESTAMP
            value: "1"
          image: ${DB_IMAGE_NAME}:${DB_IMAGE_TAG}
          imagePullPolicy: Never
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 3306
            timeoutSeconds: 1
          name: mariadb
          ports:
          - containerPort: 3306
            protocol: TCP
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 5
            periodSeconds: 3
            successThreshold: 1
            tcpSocket:
              port: 3306
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    labels:
      app: airflow-web
    name: airflow-web
  spec:
    host: airflow-web-${AIRFLOW_NAMESPACE}.poc.mydomain.com
    port:
      targetPort: web
    to:
      kind: Service
      name: airflow-web
      weight: 100

parameters:
- name: AIRFLOW_HOME
  displayName: Airflow Home
  required: true
  value: /opt/airflow
- name: AIRFLOW_IMAGE_NAME
  displayName: Airflow Image Name
  required: true
- name: AIRFLOW_IMAGE_TAG
  displayName: Airflow Image Tag
  required: true
- name: AIRFLOW_NAMESPACE
  displayName: Airflow Namespace
  required: true
- name: DB_IMAGE_NAME
  displayName: Mariadb Image Name
  required: true
- name: DB_IMAGE_TAG
  displayName: Mariadb Image Tag
  required: true
- name: DB_HOST
  displayName: Mariadb Host
  required: true
  value: mariadb
- name: DB_USER
  displayName: Mariadb User
  required: true
  value: airflow
- name: DB_PASSWORD
  displayName: Mariadb Password
  required: true
  value: airflow
- name: DB_AIRFLOW
  displayName: Mariadb Airflow Database Name
  required: true
  value: airflow
